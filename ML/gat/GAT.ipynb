{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cu113.html"
      ],
      "metadata": {
        "id": "Pqo-LfB5OMl3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73be27fd-bb0f-44de-d7c0-6ed46f804c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 33.2 MB/s \n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_sparse-0.6.15-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 43.0 MB/s \n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 45.1 MB/s \n",
            "\u001b[?25hCollecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (709 kB)\n",
            "\u001b[K     |████████████████████████████████| 709 kB 62.5 MB/s \n",
            "\u001b[?25hCollecting torch-geometric\n",
            "  Downloading torch_geometric-2.1.0.post1.tar.gz (467 kB)\n",
            "\u001b[K     |████████████████████████████████| 467 kB 27.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.1.0.post1-py3-none-any.whl size=689859 sha256=08b3d967559eba9f2e3cddce79b73124b45ea5b4c2b0a51cfab1524039fc9376\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/cb/43/f7f2e472de4d7cff31bceddadc36d634e1e545fbc17961c282\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-spline-conv, torch-sparse, torch-scatter, torch-geometric, torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0 torch-geometric-2.1.0.post1 torch-scatter-2.0.9 torch-sparse-0.6.15 torch-spline-conv-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x__gH8YQOClJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba864518-d948-49b2-ed14-f9952b6bcb4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HeteroData(\n",
            "  \u001b[1mmovie\u001b[0m={ x=[2000, 768] },\n",
            "  \u001b[1mactor\u001b[0m={ x=[7784, 768] },\n",
            "  \u001b[1mdirector\u001b[0m={ x=[1205, 768] },\n",
            "  \u001b[1mage\u001b[0m={ x=[12, 768] },\n",
            "  \u001b[1mgenre\u001b[0m={ x=[22, 768] },\n",
            "  \u001b[1mnation\u001b[0m={ x=[50, 768] }\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.data import HeteroData\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "data = HeteroData()\n",
        "\n",
        "path = \"/content/drive/MyDrive/MovieRecommendation/GAT/data\"\n",
        "\n",
        "data['movie'].x  = torch.from_numpy(np.load(path+'/embed_movie.npy'))\n",
        "data['actor'].x  = torch.from_numpy(np.load(path+'/embed_actor.npy'))\n",
        "data['director'].x  = torch.from_numpy(np.load(path+'/embed_director.npy'))\n",
        "data['age'].x  = torch.from_numpy(np.load(path+'/embed_age.npy'))\n",
        "data['genre'].x  = torch.from_numpy(np.load(path+'/embed_genre.npy'))\n",
        "data['nation'].x  = torch.from_numpy(np.load(path+'/embed_nation.npy'))\n",
        "\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "age = pd.read_csv(path+'/edge_is_rated_age.txt', sep = ' ')\n",
        "age = np.array(age.values.tolist())\n",
        "age = torch.tensor(age.astype(np.int)).t().contiguous()\n",
        "data['movie', 'is_rated_age', 'age'].edge_index = age\n",
        "\n",
        "act = pd.read_csv(path+'/edge_act.txt', sep = ' ')\n",
        "act = np.array(act.values.tolist())\n",
        "act = torch.tensor(act.astype(np.int)).t().contiguous()\n",
        "data['actor', 'act', 'movie'].edge_index = act\n",
        "\n",
        "direct = pd.read_csv(path+'/edge_direct.txt', sep = ' ')\n",
        "direct = np.array(direct.values.tolist())\n",
        "direct = torch.tensor(direct.astype(np.int)).t().contiguous()\n",
        "data['director', 'direct', 'movie'].edge_index = direct\n",
        "\n",
        "genre = pd.read_csv(path+'/edge_is_in_genre.txt', sep = ' ')\n",
        "genre = np.array(genre.values.tolist())\n",
        "genre = torch.tensor(genre.astype(np.int)).t().contiguous()\n",
        "data['movie', 'is_in_genre', 'genre'].edge_index = genre\n",
        "\n",
        "nation = pd.read_csv(path+'/edge_made_in_nation.txt', sep = ' ')\n",
        "nation = np.array(nation.values.tolist())\n",
        "nation = torch.tensor(nation.astype(np.int)).t().contiguous()\n",
        "data['movie', 'made_in_nation', 'nation'].edge_index = nation\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_EU3elLcnk_",
        "outputId": "2f3a2fb7-24e9-4e66-cf06-55f45b2c7556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HeteroData(\n",
            "  \u001b[1mmovie\u001b[0m={ x=[2000, 768] },\n",
            "  \u001b[1mactor\u001b[0m={ x=[7784, 768] },\n",
            "  \u001b[1mdirector\u001b[0m={ x=[1205, 768] },\n",
            "  \u001b[1mage\u001b[0m={ x=[12, 768] },\n",
            "  \u001b[1mgenre\u001b[0m={ x=[22, 768] },\n",
            "  \u001b[1mnation\u001b[0m={ x=[50, 768] },\n",
            "  \u001b[1m(movie, is_rated_age, age)\u001b[0m={ edge_index=[2, 2961] },\n",
            "  \u001b[1m(actor, act, movie)\u001b[0m={ edge_index=[2, 14418] },\n",
            "  \u001b[1m(director, direct, movie)\u001b[0m={ edge_index=[2, 1994] },\n",
            "  \u001b[1m(movie, is_in_genre, genre)\u001b[0m={ edge_index=[2, 4463] },\n",
            "  \u001b[1m(movie, made_in_nation, nation)\u001b[0m={ edge_index=[2, 2479] }\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "label = pd.read_csv(path+'/label_list.txt', sep = '\\t')\n",
        "movie_label = pd.read_csv(path+'/label_genre.txt', sep = '\\t')\n",
        "temp = list(map(int, re.sub(r\"[\\[\\]]\", \"\", label[\"labels\"][1]).split(\", \")))\n",
        "temp = [1 if i in temp else 0 for i in range(22)]\n",
        "# print(len(temp))\n",
        "# print(temp)\n",
        "temp = []\n",
        "for l in label[\"labels\"]:\n",
        "  t = re.sub(r\"[\\[\\]]\", \"\", l).split(\", \")\n",
        "  if t[0] != \"\":\n",
        "    t = list(map(int, t))\n",
        "  else:\n",
        "    t = []\n",
        "  t = [1 if i in t else 0 for i in range(22)]\n",
        "  temp.append(t)\n",
        "\n",
        "# print(temp)\n",
        "\n",
        "answer = []\n",
        "for lab in movie_label[\"label\"]:\n",
        "  answer.append(temp[int(lab)])\n",
        "\n",
        "\n",
        "answer = torch.tensor(answer)\n",
        "print(answer)\n",
        "data[\"movie\"].y = answer\n",
        "# print(answer)\n",
        "# print(answer.shape)\n",
        "\n",
        "# temp = [list(map(int, re.sub(r\"[\\[\\]]\", \"\", l).split(\", \"))) for l in label[\"labels\"]]\n",
        "# print(temp)\n",
        "\n",
        "# label = np.array(label.values.tolist())\n",
        "# label = torch.tensor(label.astype(np.int)).t().contiguous()[1]\n",
        "\n",
        "print(data)\n",
        "# print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cnWn3tqkZfm",
        "outputId": "3732d2b2-89f8-4cdb-aa9a-5720c5f3a76c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 1, 1,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 1,  ..., 0, 0, 0]])\n",
            "HeteroData(\n",
            "  \u001b[1mmovie\u001b[0m={\n",
            "    x=[2000, 768],\n",
            "    y=[2000, 22]\n",
            "  },\n",
            "  \u001b[1mactor\u001b[0m={ x=[7784, 768] },\n",
            "  \u001b[1mdirector\u001b[0m={ x=[1205, 768] },\n",
            "  \u001b[1mage\u001b[0m={ x=[12, 768] },\n",
            "  \u001b[1mgenre\u001b[0m={ x=[22, 768] },\n",
            "  \u001b[1mnation\u001b[0m={ x=[50, 768] },\n",
            "  \u001b[1m(movie, is_rated_age, age)\u001b[0m={ edge_index=[2, 2961] },\n",
            "  \u001b[1m(actor, act, movie)\u001b[0m={ edge_index=[2, 14418] },\n",
            "  \u001b[1m(director, direct, movie)\u001b[0m={ edge_index=[2, 1994] },\n",
            "  \u001b[1m(movie, is_in_genre, genre)\u001b[0m={ edge_index=[2, 4463] },\n",
            "  \u001b[1m(movie, made_in_nation, nation)\u001b[0m={ edge_index=[2, 2479] }\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JrBeimNrY_4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "temp = [i for i in range(2000)]\n",
        "random.shuffle(temp)\n",
        "\n",
        "train_mask = [False for i in range(2000)]\n",
        "valid_mask = [False for i in range(2000)]\n",
        "test_mask = [False for i in range(2000)]\n",
        "\n",
        "for j, i in enumerate(temp):\n",
        "  if j < 1600:\n",
        "    train_mask[i] = True\n",
        "  elif j < 1800:\n",
        "    valid_mask[i] = True\n",
        "  else:\n",
        "    test_mask[i] = True\n",
        "\n",
        "\n",
        "data[\"movie\"].train_mask = torch.tensor(train_mask)\n",
        "data[\"movie\"].val_mask = torch.tensor(valid_mask)\n",
        "data[\"movie\"].test_mask = torch.tensor(test_mask)\n",
        "\n",
        "NUMCLASS = 22\n",
        "print(NUMCLASS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR_1vXJrxwRM",
        "outputId": "242c9e2c-1b68-4ee7-8653-596110142302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_mask)\n",
        "print(valid_mask)\n",
        "print(test_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1YFcr257iUJ",
        "outputId": "e1f28d5c-f5a0-40ec-b6ce-82bdd54e2b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[True, False, False, True, True, True, False, True, False, False, True, False, True, True, True, False, True, True, True, False, True, True, True, True, True, False, True, True, True, True, False, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, True, True, False, True, False, True, True, True, False, False, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, False, True, True, True, True, True, True, True, True, False, True, False, True, False, True, True, True, True, True, True, False, False, True, True, True, False, True, False, False, False, False, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, False, True, True, True, False, True, False, True, True, True, True, False, True, False, False, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, False, False, False, True, True, True, True, True, True, False, True, False, True, True, False, False, True, False, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, False, False, False, False, True, False, True, True, True, False, True, True, True, True, True, False, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, False, True, True, False, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, False, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, False, True, False, True, False, True, True, False, True, True, True, True, False, True, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, False, False, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, False, True, True, True, False, True, True, True, False, True, False, True, True, True, True, False, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, False, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, False, False, True, True, True, True, True, True, False, True, False, True, True, True, False, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, False, True, True, False, True, False, True, False, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, False, False, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, False, True, False, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, False, True, False, True, True, True, True, True, True, True, False, True, True, False, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, False, True, False, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, False, False, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, False, True, False, True, True, False, True, True, True, True, True, True, True, True, False, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, False, True, True, True, True, True, True, False, False, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, False, False, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, False, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, False, False, False, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, False, False, False, True, True, True, False, False, True, True, True, True, True, True, True, False, True, True, False, True, False, True, True, True, True, True, False, True, True, False, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, False, True, True, False, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, False, False, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, False, True, True, True, True, True, False, True, True, True, True, False, True, False, True, False, True, True, True, True, True, True, True, False, True, False, True, False, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, False, False, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, False, True, True, False, False, True, True, True, True, True, True, True, False, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, False, True, True, False, True, False, False, True, True, False, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, False, False, True, True, False, True, True, False, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, False, True, True, True, False, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, False, False, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, False, True, True, False, True, True, True, True, True, True, False, False, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, False, False, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, False, True, True, False, False, True, False, False, False, True, True, False, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, False, True, True, True, False, False, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, False, False, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, False, False, True, True, True, False, True, False, True, True, False, False, True, True, False, True, True, False, False, False, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, False, True, False, True, True, False, False, False, True, True, False, True, True, False, True, True, True, True, True, False, True, False, True, True, True, True, True, False, True, False, True, False, True, False, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, False, True, True, False, True, False, True, True, False, True, False, False, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, False, False, False, False, False, True, True, True, True, False, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False]\n",
            "[False, True, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, True, False, True, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, True, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, True, False, True, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, True, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, True, False, False, False, False, False, False, False, True, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, True, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, True, False, False, False, False, True, False, True, False, True, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, True, False, False, True, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, True, True, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, False, False, True, False, False, True, False, False, False, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True]\n",
            "[False, False, False, False, False, False, False, False, True, True, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, True, False, True, False, False, False, False, True, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, True, False, True, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, True, False, True, False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, True, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True, True, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, True, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, True, False, False, True, False, True, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, True, False, False, True, True, False, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True, False, False, False, True, False, True, False, False, False, False, False, False, True, False, False, True, True, False, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, True, False, True, False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, True, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, True, True, True, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_geometric.transforms as T\n",
        "\n",
        "data = T.ToUndirected()(data)\n",
        "#data = T.AddSelfLoops()(data)\n",
        "#data = T.NormalizeFeatures()(data)\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "cya09XYsVSR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdacb2f5-015a-420c-dd7b-7d7755ad05c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HeteroData(\n",
            "  \u001b[1mmovie\u001b[0m={\n",
            "    x=[2000, 768],\n",
            "    y=[2000, 22],\n",
            "    train_mask=[2000],\n",
            "    val_mask=[2000],\n",
            "    test_mask=[2000]\n",
            "  },\n",
            "  \u001b[1mactor\u001b[0m={ x=[7784, 768] },\n",
            "  \u001b[1mdirector\u001b[0m={ x=[1205, 768] },\n",
            "  \u001b[1mage\u001b[0m={ x=[12, 768] },\n",
            "  \u001b[1mgenre\u001b[0m={ x=[22, 768] },\n",
            "  \u001b[1mnation\u001b[0m={ x=[50, 768] },\n",
            "  \u001b[1m(movie, is_rated_age, age)\u001b[0m={ edge_index=[2, 2961] },\n",
            "  \u001b[1m(actor, act, movie)\u001b[0m={ edge_index=[2, 14418] },\n",
            "  \u001b[1m(director, direct, movie)\u001b[0m={ edge_index=[2, 1994] },\n",
            "  \u001b[1m(movie, is_in_genre, genre)\u001b[0m={ edge_index=[2, 4463] },\n",
            "  \u001b[1m(movie, made_in_nation, nation)\u001b[0m={ edge_index=[2, 2479] },\n",
            "  \u001b[1m(age, rev_is_rated_age, movie)\u001b[0m={ edge_index=[2, 2961] },\n",
            "  \u001b[1m(movie, rev_act, actor)\u001b[0m={ edge_index=[2, 14418] },\n",
            "  \u001b[1m(movie, rev_direct, director)\u001b[0m={ edge_index=[2, 1994] },\n",
            "  \u001b[1m(genre, rev_is_in_genre, movie)\u001b[0m={ edge_index=[2, 4463] },\n",
            "  \u001b[1m(nation, rev_made_in_nation, movie)\u001b[0m={ edge_index=[2, 2479] }\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['movie'].x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRqbIuljVsTP",
        "outputId": "fc202fd5-764e-4549-9272-ee85c658db0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.0061e-02,  1.1399e-02, -4.9557e-01,  ..., -5.7112e-02,\n",
            "          1.4089e-02,  3.9966e-02],\n",
            "        [ 1.7557e-02,  1.4669e-02, -6.2961e-01,  ..., -3.6282e-02,\n",
            "          5.1188e-02, -2.7295e-02],\n",
            "        [ 3.4202e-02, -1.1871e-02, -3.1600e-01,  ...,  2.5435e-02,\n",
            "         -5.0507e-04,  4.6632e-02],\n",
            "        ...,\n",
            "        [ 2.6428e-03,  2.5621e-02, -4.3013e-01,  ..., -6.1772e-02,\n",
            "         -3.3588e-03,  3.9521e-02],\n",
            "        [-5.2318e-02, -1.3269e-02, -5.7703e-01,  ..., -3.4845e-02,\n",
            "         -4.8633e-02,  7.8522e-03],\n",
            "        [-5.9738e-02, -6.4434e-03, -3.6663e-01,  ...,  1.5921e-02,\n",
            "         -7.5789e-02, -1.4073e-02]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.x_dict, data.edge_index_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snKhkyU3ZH3k",
        "outputId": "179a922e-7fb5-46fd-f7e0-ce23000d74ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'movie': tensor([[ 2.0061e-02,  1.1399e-02, -4.9557e-01,  ..., -5.7112e-02,\n",
            "          1.4089e-02,  3.9966e-02],\n",
            "        [ 1.7557e-02,  1.4669e-02, -6.2961e-01,  ..., -3.6282e-02,\n",
            "          5.1188e-02, -2.7295e-02],\n",
            "        [ 3.4202e-02, -1.1871e-02, -3.1600e-01,  ...,  2.5435e-02,\n",
            "         -5.0507e-04,  4.6632e-02],\n",
            "        ...,\n",
            "        [ 2.6428e-03,  2.5621e-02, -4.3013e-01,  ..., -6.1772e-02,\n",
            "         -3.3588e-03,  3.9521e-02],\n",
            "        [-5.2318e-02, -1.3269e-02, -5.7703e-01,  ..., -3.4845e-02,\n",
            "         -4.8633e-02,  7.8522e-03],\n",
            "        [-5.9738e-02, -6.4434e-03, -3.6663e-01,  ...,  1.5921e-02,\n",
            "         -7.5789e-02, -1.4073e-02]]), 'actor': tensor([[0.2720, 0.3348, 0.1570,  ..., 0.9093, 0.1776, 0.6180],\n",
            "        [0.5908, 0.3077, 0.1083,  ..., 0.7695, 0.5961, 0.2496],\n",
            "        [0.9284, 0.5966, 0.4689,  ..., 0.7570, 0.4779, 0.0585],\n",
            "        ...,\n",
            "        [0.1817, 0.0877, 0.3339,  ..., 0.5294, 0.8213, 0.5490],\n",
            "        [0.7532, 0.9795, 0.3188,  ..., 0.9066, 0.1378, 0.7290],\n",
            "        [0.7464, 0.5008, 0.7266,  ..., 0.3150, 0.9786, 0.8147]]), 'director': tensor([[0.2222, 0.0020, 0.1905,  ..., 0.1653, 0.1806, 0.5723],\n",
            "        [0.5709, 0.4263, 0.3825,  ..., 0.1719, 0.9935, 0.0334],\n",
            "        [0.2045, 0.9362, 0.8355,  ..., 0.5776, 0.6175, 0.4382],\n",
            "        ...,\n",
            "        [0.5896, 0.4226, 0.2451,  ..., 0.7421, 0.7865, 0.9777],\n",
            "        [0.3992, 0.2950, 0.4930,  ..., 0.0020, 0.9250, 0.6063],\n",
            "        [0.7906, 0.6133, 0.2514,  ..., 0.6630, 0.2502, 0.7785]]), 'age': tensor([[0.2678, 0.4520, 0.4054,  ..., 0.7446, 0.8518, 0.7717],\n",
            "        [0.4687, 0.3599, 0.7488,  ..., 0.0071, 0.5631, 0.3646],\n",
            "        [0.8984, 0.3664, 0.1795,  ..., 0.7288, 0.9979, 0.1266],\n",
            "        ...,\n",
            "        [0.5615, 0.8685, 0.8351,  ..., 0.5123, 0.8769, 0.6598],\n",
            "        [0.0596, 0.0381, 0.4183,  ..., 0.6779, 0.9056, 0.2199],\n",
            "        [0.6905, 0.4076, 0.3730,  ..., 0.4012, 0.6703, 0.1610]]), 'genre': tensor([[0.9067, 0.0018, 0.7005,  ..., 0.4957, 0.1872, 0.7648],\n",
            "        [0.1635, 0.2190, 0.1831,  ..., 0.5197, 0.2603, 0.9093],\n",
            "        [0.3342, 0.3380, 0.3682,  ..., 0.5405, 0.1205, 0.7586],\n",
            "        ...,\n",
            "        [0.0495, 0.6196, 0.1611,  ..., 0.0977, 0.1977, 0.9782],\n",
            "        [0.1739, 0.5065, 0.1790,  ..., 0.2691, 0.5126, 0.9998],\n",
            "        [0.7182, 0.6920, 0.8943,  ..., 0.3767, 0.0572, 0.3675]]), 'nation': tensor([[0.8008, 0.7975, 0.6926,  ..., 0.8922, 0.3050, 0.8319],\n",
            "        [0.7572, 0.1747, 0.1876,  ..., 0.6728, 0.9547, 0.5354],\n",
            "        [0.6468, 0.0742, 0.2332,  ..., 0.1352, 0.4306, 0.9097],\n",
            "        ...,\n",
            "        [0.6070, 0.3174, 0.8256,  ..., 0.7627, 0.0090, 0.2934],\n",
            "        [0.7414, 0.8547, 0.6061,  ..., 0.9202, 0.7222, 0.2183],\n",
            "        [0.4462, 0.6242, 0.5083,  ..., 0.5113, 0.7781, 0.2518]])} {('movie', 'is_rated_age', 'age'): tensor([[   0,    1,    2,  ..., 1998, 1999, 1999],\n",
            "        [   0,    1,    0,  ...,    2,    9,    4]]), ('actor', 'act', 'movie'): tensor([[   0,    1,    2,  ..., 7782, 3056, 7783],\n",
            "        [   0,    0,    0,  ..., 1999, 1999, 1999]]), ('director', 'direct', 'movie'): tensor([[   0,    1,    2,  ...,   30, 1203, 1204],\n",
            "        [   0,    1,    2,  ..., 1997, 1998, 1999]]), ('movie', 'is_in_genre', 'genre'): tensor([[   0,    1,    1,  ..., 1998, 1999, 1999],\n",
            "        [   0,    1,    2,  ...,    7,   11,    2]]), ('movie', 'made_in_nation', 'nation'): tensor([[   0,    1,    1,  ..., 1999, 1999, 1999],\n",
            "        [   0,    1,    2,  ...,    2,    0,   18]]), ('age', 'rev_is_rated_age', 'movie'): tensor([[   0,    1,    0,  ...,    2,    9,    4],\n",
            "        [   0,    1,    2,  ..., 1998, 1999, 1999]]), ('movie', 'rev_act', 'actor'): tensor([[   0,    0,    0,  ..., 1999, 1999, 1999],\n",
            "        [   0,    1,    2,  ..., 7782, 3056, 7783]]), ('movie', 'rev_direct', 'director'): tensor([[   0,    1,    2,  ..., 1997, 1998, 1999],\n",
            "        [   0,    1,    2,  ...,   30, 1203, 1204]]), ('genre', 'rev_is_in_genre', 'movie'): tensor([[   0,    1,    2,  ...,    7,   11,    2],\n",
            "        [   0,    1,    1,  ..., 1998, 1999, 1999]]), ('nation', 'rev_made_in_nation', 'movie'): tensor([[   0,    1,    2,  ...,    2,    0,   18],\n",
            "        [   0,    1,    1,  ..., 1999, 1999, 1999]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GATConv, Linear, to_hetero\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import OGB_MAG\n",
        "from torch_geometric.nn import SAGEConv, to_hetero\n",
        "\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
        "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "model = GNN(hidden_channels=64, out_channels=22)\n",
        "model = to_hetero(model, data.metadata(), aggr='sum')\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GATConv(768, hidden_channels[0], add_self_loops=False, dropout=0.6)\n",
        "        self.lin1 = Linear(-1, hidden_channels[0])\n",
        "        self.conv2 = GATConv(hidden_channels[0], hidden_channels[1], add_self_loops=False, dropout=0.6)\n",
        "        self.lin2 = Linear(-1, hidden_channels[1])\n",
        "        self.conv3 = GATConv(hidden_channels[1], out_channels, add_self_loops=False, dropout=0.6)\n",
        "        self.lin3 = Linear(-1, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv1(x, edge_index) + self.lin1(x)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv2(x, edge_index) + self.lin2(x)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv3(x, edge_index) + self.lin3(x)\n",
        "        x = x.sigmoid()\n",
        "        return x\n",
        "\n",
        "# model = GAT(hidden_channels=[384, 96], out_channels=NUMCLASS)\n",
        "# model = to_hetero(model, data.metadata(), aggr='sum')"
      ],
      "metadata": {
        "id": "JTFpw_KF3tdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "data = data.to(device)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "\n",
        "with torch.no_grad():  # Initialize lazy modules.\n",
        "     out = model(data.x_dict, data.edge_index_dict)\n",
        "\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x_dict, data.edge_index_dict)\n",
        "    mask = data['movie'].train_mask\n",
        "    loss = criterion(out['movie'][mask], data['movie'].y[mask].float())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "losses = []\n",
        "losses2 = []\n",
        "model.train()\n",
        "vlossL = 999\n",
        "for epoch in range(400):\n",
        "    loss = train()\n",
        "    losses.append(loss)\n",
        "    with torch.no_grad():\n",
        "      out = model(data.x_dict, data.edge_index_dict)\n",
        "      mask = data['movie'].val_mask.to(device)\n",
        "      vloss = criterion(out['movie'][mask], data['movie'].y[mask].float())\n",
        "      losses2.append(vloss.detach().cpu())\n",
        "    print(f\"{epoch}-epoch and {loss}-train_loss {vloss}-valid_loss\\n\")\n",
        "    if(vloss > vlossL and vloss < 0.005):break\n",
        "    else:vlossL = vloss\n",
        "    "
      ],
      "metadata": {
        "id": "KL1IWyGf3593",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430bb31b-4023-4b52-dc7e-43e956608f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0-epoch and 0.7928274869918823-train_loss 4.004863262176514-valid_loss\n",
            "\n",
            "1-epoch and 4.489349365234375-train_loss 6.408749103546143-valid_loss\n",
            "\n",
            "2-epoch and 6.297093391418457-train_loss 4.728341102600098-valid_loss\n",
            "\n",
            "3-epoch and 4.605349063873291-train_loss 7.054765701293945-valid_loss\n",
            "\n",
            "4-epoch and 6.996011257171631-train_loss 4.202011585235596-valid_loss\n",
            "\n",
            "5-epoch and 4.274713039398193-train_loss 2.7160794734954834-valid_loss\n",
            "\n",
            "6-epoch and 2.8457016944885254-train_loss 3.166031837463379-valid_loss\n",
            "\n",
            "7-epoch and 3.277921199798584-train_loss 2.5446817874908447-valid_loss\n",
            "\n",
            "8-epoch and 2.6348354816436768-train_loss 1.936611294746399-valid_loss\n",
            "\n",
            "9-epoch and 2.0423455238342285-train_loss 1.42731773853302-valid_loss\n",
            "\n",
            "10-epoch and 1.581735610961914-train_loss 1.4774041175842285-valid_loss\n",
            "\n",
            "11-epoch and 1.683626651763916-train_loss 1.3371950387954712-valid_loss\n",
            "\n",
            "12-epoch and 1.5545070171356201-train_loss 1.0092871189117432-valid_loss\n",
            "\n",
            "13-epoch and 1.2104839086532593-train_loss 0.9261347055435181-valid_loss\n",
            "\n",
            "14-epoch and 1.1024013757705688-train_loss 1.2942711114883423-valid_loss\n",
            "\n",
            "15-epoch and 1.4363805055618286-train_loss 0.9910726547241211-valid_loss\n",
            "\n",
            "16-epoch and 1.1045159101486206-train_loss 0.8530141711235046-valid_loss\n",
            "\n",
            "17-epoch and 0.9520794153213501-train_loss 0.8032187819480896-valid_loss\n",
            "\n",
            "18-epoch and 0.8946216702461243-train_loss 0.8470078110694885-valid_loss\n",
            "\n",
            "19-epoch and 0.921179473400116-train_loss 0.6979297995567322-valid_loss\n",
            "\n",
            "20-epoch and 0.7716363072395325-train_loss 0.6048958897590637-valid_loss\n",
            "\n",
            "21-epoch and 0.6878073215484619-train_loss 0.5196182727813721-valid_loss\n",
            "\n",
            "22-epoch and 0.5954942107200623-train_loss 0.38752418756484985-valid_loss\n",
            "\n",
            "23-epoch and 0.4477974474430084-train_loss 0.5586346983909607-valid_loss\n",
            "\n",
            "24-epoch and 0.5956280827522278-train_loss 0.34252214431762695-valid_loss\n",
            "\n",
            "25-epoch and 0.37414178252220154-train_loss 0.35700228810310364-valid_loss\n",
            "\n",
            "26-epoch and 0.38111722469329834-train_loss 0.3558492362499237-valid_loss\n",
            "\n",
            "27-epoch and 0.37297141551971436-train_loss 0.3060232996940613-valid_loss\n",
            "\n",
            "28-epoch and 0.3278822898864746-train_loss 0.2672751843929291-valid_loss\n",
            "\n",
            "29-epoch and 0.2952834963798523-train_loss 0.23167964816093445-valid_loss\n",
            "\n",
            "30-epoch and 0.26183754205703735-train_loss 0.22875544428825378-valid_loss\n",
            "\n",
            "31-epoch and 0.2642527222633362-train_loss 0.2028404176235199-valid_loss\n",
            "\n",
            "32-epoch and 0.23199890553951263-train_loss 0.20132549107074738-valid_loss\n",
            "\n",
            "33-epoch and 0.225629061460495-train_loss 0.18096546828746796-valid_loss\n",
            "\n",
            "34-epoch and 0.20262525975704193-train_loss 0.18113550543785095-valid_loss\n",
            "\n",
            "35-epoch and 0.20290471613407135-train_loss 0.17181549966335297-valid_loss\n",
            "\n",
            "36-epoch and 0.18946395814418793-train_loss 0.16692125797271729-valid_loss\n",
            "\n",
            "37-epoch and 0.1811663806438446-train_loss 0.16197270154953003-valid_loss\n",
            "\n",
            "38-epoch and 0.17525944113731384-train_loss 0.1523963361978531-valid_loss\n",
            "\n",
            "39-epoch and 0.16720622777938843-train_loss 0.1472775787115097-valid_loss\n",
            "\n",
            "40-epoch and 0.1636713743209839-train_loss 0.14759300649166107-valid_loss\n",
            "\n",
            "41-epoch and 0.1642475128173828-train_loss 0.1462676227092743-valid_loss\n",
            "\n",
            "42-epoch and 0.16266360878944397-train_loss 0.1415782868862152-valid_loss\n",
            "\n",
            "43-epoch and 0.15754364430904388-train_loss 0.139730304479599-valid_loss\n",
            "\n",
            "44-epoch and 0.1540154218673706-train_loss 0.1388687938451767-valid_loss\n",
            "\n",
            "45-epoch and 0.15171362459659576-train_loss 0.1358862817287445-valid_loss\n",
            "\n",
            "46-epoch and 0.14829100668430328-train_loss 0.13305358588695526-valid_loss\n",
            "\n",
            "47-epoch and 0.1453959196805954-train_loss 0.13083520531654358-valid_loss\n",
            "\n",
            "48-epoch and 0.14295728504657745-train_loss 0.12814603745937347-valid_loss\n",
            "\n",
            "49-epoch and 0.14058904349803925-train_loss 0.12620702385902405-valid_loss\n",
            "\n",
            "50-epoch and 0.1380942016839981-train_loss 0.12470908463001251-valid_loss\n",
            "\n",
            "51-epoch and 0.13578492403030396-train_loss 0.1241212859749794-valid_loss\n",
            "\n",
            "52-epoch and 0.13368578255176544-train_loss 0.12613342702388763-valid_loss\n",
            "\n",
            "53-epoch and 0.13579031825065613-train_loss 0.12384116649627686-valid_loss\n",
            "\n",
            "54-epoch and 0.13355638086795807-train_loss 0.1204543262720108-valid_loss\n",
            "\n",
            "55-epoch and 0.13092578947544098-train_loss 0.11752840876579285-valid_loss\n",
            "\n",
            "56-epoch and 0.12675903737545013-train_loss 0.1254335343837738-valid_loss\n",
            "\n",
            "57-epoch and 0.13282205164432526-train_loss 0.11651023477315903-valid_loss\n",
            "\n",
            "58-epoch and 0.12671123445034027-train_loss 0.11587878316640854-valid_loss\n",
            "\n",
            "59-epoch and 0.1269395351409912-train_loss 0.11278192698955536-valid_loss\n",
            "\n",
            "60-epoch and 0.1219434142112732-train_loss 0.11668512225151062-valid_loss\n",
            "\n",
            "61-epoch and 0.12380681931972504-train_loss 0.10993795096874237-valid_loss\n",
            "\n",
            "62-epoch and 0.1176491230726242-train_loss 0.11013544350862503-valid_loss\n",
            "\n",
            "63-epoch and 0.11833319813013077-train_loss 0.10923061519861221-valid_loss\n",
            "\n",
            "64-epoch and 0.11572767794132233-train_loss 0.10765863955020905-valid_loss\n",
            "\n",
            "65-epoch and 0.1139201819896698-train_loss 0.10532142221927643-valid_loss\n",
            "\n",
            "66-epoch and 0.11245821416378021-train_loss 0.10371346026659012-valid_loss\n",
            "\n",
            "67-epoch and 0.11108614504337311-train_loss 0.10257941484451294-valid_loss\n",
            "\n",
            "68-epoch and 0.1096673458814621-train_loss 0.10056610405445099-valid_loss\n",
            "\n",
            "69-epoch and 0.1079263687133789-train_loss 0.09910007566213608-valid_loss\n",
            "\n",
            "70-epoch and 0.1066385880112648-train_loss 0.09823837131261826-valid_loss\n",
            "\n",
            "71-epoch and 0.10435100644826889-train_loss 0.09780362248420715-valid_loss\n",
            "\n",
            "72-epoch and 0.10307451337575912-train_loss 0.09411496669054031-valid_loss\n",
            "\n",
            "73-epoch and 0.10058765113353729-train_loss 0.09192145615816116-valid_loss\n",
            "\n",
            "74-epoch and 0.09845434129238129-train_loss 0.09028122574090958-valid_loss\n",
            "\n",
            "75-epoch and 0.09618579596281052-train_loss 0.08765007555484772-valid_loss\n",
            "\n",
            "76-epoch and 0.09416352212429047-train_loss 0.08730322867631912-valid_loss\n",
            "\n",
            "77-epoch and 0.09296439588069916-train_loss 0.08568714559078217-valid_loss\n",
            "\n",
            "78-epoch and 0.09329841285943985-train_loss 0.08278457075357437-valid_loss\n",
            "\n",
            "79-epoch and 0.08852165937423706-train_loss 0.08096922188997269-valid_loss\n",
            "\n",
            "80-epoch and 0.08642792701721191-train_loss 0.07744994014501572-valid_loss\n",
            "\n",
            "81-epoch and 0.08413709700107574-train_loss 0.07535646110773087-valid_loss\n",
            "\n",
            "82-epoch and 0.081311896443367-train_loss 0.07414255291223526-valid_loss\n",
            "\n",
            "83-epoch and 0.07950910925865173-train_loss 0.07202419638633728-valid_loss\n",
            "\n",
            "84-epoch and 0.0771757960319519-train_loss 0.07073007524013519-valid_loss\n",
            "\n",
            "85-epoch and 0.07528135925531387-train_loss 0.07020947337150574-valid_loss\n",
            "\n",
            "86-epoch and 0.07499182224273682-train_loss 0.07100699841976166-valid_loss\n",
            "\n",
            "87-epoch and 0.07675959169864655-train_loss 0.06776544451713562-valid_loss\n",
            "\n",
            "88-epoch and 0.07278073579072952-train_loss 0.06621593981981277-valid_loss\n",
            "\n",
            "89-epoch and 0.0709264948964119-train_loss 0.06554536521434784-valid_loss\n",
            "\n",
            "90-epoch and 0.0721106082201004-train_loss 0.06476765125989914-valid_loss\n",
            "\n",
            "91-epoch and 0.070683054625988-train_loss 0.06546127796173096-valid_loss\n",
            "\n",
            "92-epoch and 0.06845033913850784-train_loss 0.0636126846075058-valid_loss\n",
            "\n",
            "93-epoch and 0.06596661359071732-train_loss 0.059372931718826294-valid_loss\n",
            "\n",
            "94-epoch and 0.06369874626398087-train_loss 0.057733338326215744-valid_loss\n",
            "\n",
            "95-epoch and 0.06332603842020035-train_loss 0.05683844909071922-valid_loss\n",
            "\n",
            "96-epoch and 0.061606474220752716-train_loss 0.056959230452775955-valid_loss\n",
            "\n",
            "97-epoch and 0.06070340797305107-train_loss 0.05469333752989769-valid_loss\n",
            "\n",
            "98-epoch and 0.05847322568297386-train_loss 0.05451199784874916-valid_loss\n",
            "\n",
            "99-epoch and 0.059061501175165176-train_loss 0.05248492956161499-valid_loss\n",
            "\n",
            "100-epoch and 0.05585223436355591-train_loss 0.05240504816174507-valid_loss\n",
            "\n",
            "101-epoch and 0.05535447970032692-train_loss 0.05059584975242615-valid_loss\n",
            "\n",
            "102-epoch and 0.05467253178358078-train_loss 0.05096781626343727-valid_loss\n",
            "\n",
            "103-epoch and 0.055896129459142685-train_loss 0.053229957818984985-valid_loss\n",
            "\n",
            "104-epoch and 0.0573476105928421-train_loss 0.04966753348708153-valid_loss\n",
            "\n",
            "105-epoch and 0.05181059613823891-train_loss 0.05091288685798645-valid_loss\n",
            "\n",
            "106-epoch and 0.05346599966287613-train_loss 0.050782885402441025-valid_loss\n",
            "\n",
            "107-epoch and 0.05496916547417641-train_loss 0.045425381511449814-valid_loss\n",
            "\n",
            "108-epoch and 0.049270402640104294-train_loss 0.05337591469287872-valid_loss\n",
            "\n",
            "109-epoch and 0.055872056633234024-train_loss 0.05025361105799675-valid_loss\n",
            "\n",
            "110-epoch and 0.05327453091740608-train_loss 0.05045373737812042-valid_loss\n",
            "\n",
            "111-epoch and 0.05294523015618324-train_loss 0.04845805838704109-valid_loss\n",
            "\n",
            "112-epoch and 0.051523011177778244-train_loss 0.04503658413887024-valid_loss\n",
            "\n",
            "113-epoch and 0.048725638538599014-train_loss 0.04609599709510803-valid_loss\n",
            "\n",
            "114-epoch and 0.049189526587724686-train_loss 0.04493401199579239-valid_loss\n",
            "\n",
            "115-epoch and 0.047891438007354736-train_loss 0.04357190057635307-valid_loss\n",
            "\n",
            "116-epoch and 0.04704528674483299-train_loss 0.04240629822015762-valid_loss\n",
            "\n",
            "117-epoch and 0.04600552096962929-train_loss 0.04312707483768463-valid_loss\n",
            "\n",
            "118-epoch and 0.04518025740981102-train_loss 0.043555181473493576-valid_loss\n",
            "\n",
            "119-epoch and 0.04513094946742058-train_loss 0.040140971541404724-valid_loss\n",
            "\n",
            "120-epoch and 0.04285728558897972-train_loss 0.04020436480641365-valid_loss\n",
            "\n",
            "121-epoch and 0.04392418637871742-train_loss 0.04019875079393387-valid_loss\n",
            "\n",
            "122-epoch and 0.04211807996034622-train_loss 0.041404642164707184-valid_loss\n",
            "\n",
            "123-epoch and 0.04273767024278641-train_loss 0.0385892353951931-valid_loss\n",
            "\n",
            "124-epoch and 0.04190476983785629-train_loss 0.03927665576338768-valid_loss\n",
            "\n",
            "125-epoch and 0.04143371433019638-train_loss 0.03973057121038437-valid_loss\n",
            "\n",
            "126-epoch and 0.04058215022087097-train_loss 0.03882277011871338-valid_loss\n",
            "\n",
            "127-epoch and 0.04031633213162422-train_loss 0.03739691525697708-valid_loss\n",
            "\n",
            "128-epoch and 0.03978664427995682-train_loss 0.037239961326122284-valid_loss\n",
            "\n",
            "129-epoch and 0.0398331880569458-train_loss 0.03710619732737541-valid_loss\n",
            "\n",
            "130-epoch and 0.03879088535904884-train_loss 0.03780919313430786-valid_loss\n",
            "\n",
            "131-epoch and 0.03889121487736702-train_loss 0.03646167367696762-valid_loss\n",
            "\n",
            "132-epoch and 0.03827224671840668-train_loss 0.03610372170805931-valid_loss\n",
            "\n",
            "133-epoch and 0.03788663446903229-train_loss 0.03657051920890808-valid_loss\n",
            "\n",
            "134-epoch and 0.03759898617863655-train_loss 0.036117635667324066-valid_loss\n",
            "\n",
            "135-epoch and 0.03734381124377251-train_loss 0.03509474918246269-valid_loss\n",
            "\n",
            "136-epoch and 0.03683585301041603-train_loss 0.03457535430788994-valid_loss\n",
            "\n",
            "137-epoch and 0.03651664778590202-train_loss 0.03481616452336311-valid_loss\n",
            "\n",
            "138-epoch and 0.036234427243471146-train_loss 0.035508424043655396-valid_loss\n",
            "\n",
            "139-epoch and 0.03625151142477989-train_loss 0.03413734585046768-valid_loss\n",
            "\n",
            "140-epoch and 0.03556691110134125-train_loss 0.03384711593389511-valid_loss\n",
            "\n",
            "141-epoch and 0.0355195626616478-train_loss 0.03982740268111229-valid_loss\n",
            "\n",
            "142-epoch and 0.04061177372932434-train_loss 0.047436196357011795-valid_loss\n",
            "\n",
            "143-epoch and 0.05013183876872063-train_loss 0.04233591631054878-valid_loss\n",
            "\n",
            "144-epoch and 0.04429451748728752-train_loss 0.0457063652575016-valid_loss\n",
            "\n",
            "145-epoch and 0.047810494899749756-train_loss 0.039051998406648636-valid_loss\n",
            "\n",
            "146-epoch and 0.03999848663806915-train_loss 0.04028915613889694-valid_loss\n",
            "\n",
            "147-epoch and 0.04237670451402664-train_loss 0.03674650192260742-valid_loss\n",
            "\n",
            "148-epoch and 0.03967296704649925-train_loss 0.03903023153543472-valid_loss\n",
            "\n",
            "149-epoch and 0.04290524125099182-train_loss 0.037874557077884674-valid_loss\n",
            "\n",
            "150-epoch and 0.04173494502902031-train_loss 0.034996435046195984-valid_loss\n",
            "\n",
            "151-epoch and 0.039221614599227905-train_loss 0.034664541482925415-valid_loss\n",
            "\n",
            "152-epoch and 0.038423843681812286-train_loss 0.0347071997821331-valid_loss\n",
            "\n",
            "153-epoch and 0.036873556673526764-train_loss 0.03583347424864769-valid_loss\n",
            "\n",
            "154-epoch and 0.03645454719662666-train_loss 0.03492821007966995-valid_loss\n",
            "\n",
            "155-epoch and 0.03596099093556404-train_loss 0.03267752751708031-valid_loss\n",
            "\n",
            "156-epoch and 0.03466085344552994-train_loss 0.03280544653534889-valid_loss\n",
            "\n",
            "157-epoch and 0.03426056727766991-train_loss 0.03457676246762276-valid_loss\n",
            "\n",
            "158-epoch and 0.03485046327114105-train_loss 0.03249875828623772-valid_loss\n",
            "\n",
            "159-epoch and 0.032717932015657425-train_loss 0.03157424181699753-valid_loss\n",
            "\n",
            "160-epoch and 0.03315442055463791-train_loss 0.03219147026538849-valid_loss\n",
            "\n",
            "161-epoch and 0.03324919193983078-train_loss 0.031668853014707565-valid_loss\n",
            "\n",
            "162-epoch and 0.032694585621356964-train_loss 0.030859114602208138-valid_loss\n",
            "\n",
            "163-epoch and 0.03243531659245491-train_loss 0.031035538762807846-valid_loss\n",
            "\n",
            "164-epoch and 0.032293859869241714-train_loss 0.030602145940065384-valid_loss\n",
            "\n",
            "165-epoch and 0.03192564472556114-train_loss 0.029941655695438385-valid_loss\n",
            "\n",
            "166-epoch and 0.03169824928045273-train_loss 0.030469929799437523-valid_loss\n",
            "\n",
            "167-epoch and 0.031401701271533966-train_loss 0.030201083049178123-valid_loss\n",
            "\n",
            "168-epoch and 0.030810276046395302-train_loss 0.030000103637576103-valid_loss\n",
            "\n",
            "169-epoch and 0.03101387433707714-train_loss 0.030292119830846786-valid_loss\n",
            "\n",
            "170-epoch and 0.030592000111937523-train_loss 0.03019218146800995-valid_loss\n",
            "\n",
            "171-epoch and 0.030528059229254723-train_loss 0.02923124097287655-valid_loss\n",
            "\n",
            "172-epoch and 0.03006730228662491-train_loss 0.02873620204627514-valid_loss\n",
            "\n",
            "173-epoch and 0.02973005175590515-train_loss 0.029002869501709938-valid_loss\n",
            "\n",
            "174-epoch and 0.029872717335820198-train_loss 0.028466666117310524-valid_loss\n",
            "\n",
            "175-epoch and 0.02962375245988369-train_loss 0.0283817145973444-valid_loss\n",
            "\n",
            "176-epoch and 0.029572632163763046-train_loss 0.02895280532538891-valid_loss\n",
            "\n",
            "177-epoch and 0.029217764735221863-train_loss 0.02848907746374607-valid_loss\n",
            "\n",
            "178-epoch and 0.02910669520497322-train_loss 0.027937861159443855-valid_loss\n",
            "\n",
            "179-epoch and 0.028919896110892296-train_loss 0.028763968497514725-valid_loss\n",
            "\n",
            "180-epoch and 0.028848322108387947-train_loss 0.028061246499419212-valid_loss\n",
            "\n",
            "181-epoch and 0.028466664254665375-train_loss 0.02740464359521866-valid_loss\n",
            "\n",
            "182-epoch and 0.02849547751247883-train_loss 0.027969516813755035-valid_loss\n",
            "\n",
            "183-epoch and 0.02871743217110634-train_loss 0.030055556446313858-valid_loss\n",
            "\n",
            "184-epoch and 0.030249429866671562-train_loss 0.027985623106360435-valid_loss\n",
            "\n",
            "185-epoch and 0.02966715395450592-train_loss 0.029625659808516502-valid_loss\n",
            "\n",
            "186-epoch and 0.02997634932398796-train_loss 0.028839705511927605-valid_loss\n",
            "\n",
            "187-epoch and 0.028775673359632492-train_loss 0.027559062466025352-valid_loss\n",
            "\n",
            "188-epoch and 0.02834748663008213-train_loss 0.02741328813135624-valid_loss\n",
            "\n",
            "189-epoch and 0.027776384726166725-train_loss 0.027971500530838966-valid_loss\n",
            "\n",
            "190-epoch and 0.02777940407395363-train_loss 0.027656985446810722-valid_loss\n",
            "\n",
            "191-epoch and 0.02815200388431549-train_loss 0.026649385690689087-valid_loss\n",
            "\n",
            "192-epoch and 0.02732567861676216-train_loss 0.02746189385652542-valid_loss\n",
            "\n",
            "193-epoch and 0.027468634769320488-train_loss 0.026897171512246132-valid_loss\n",
            "\n",
            "194-epoch and 0.026975085958838463-train_loss 0.026829466223716736-valid_loss\n",
            "\n",
            "195-epoch and 0.027095921337604523-train_loss 0.027661481872200966-valid_loss\n",
            "\n",
            "196-epoch and 0.027453400194644928-train_loss 0.026545701548457146-valid_loss\n",
            "\n",
            "197-epoch and 0.026666797697544098-train_loss 0.02677437663078308-valid_loss\n",
            "\n",
            "198-epoch and 0.026635976508259773-train_loss 0.02671288512647152-valid_loss\n",
            "\n",
            "199-epoch and 0.026075325906276703-train_loss 0.02611139789223671-valid_loss\n",
            "\n",
            "200-epoch and 0.026115620508790016-train_loss 0.026124414056539536-valid_loss\n",
            "\n",
            "201-epoch and 0.02611400932073593-train_loss 0.02626791223883629-valid_loss\n",
            "\n",
            "202-epoch and 0.02591448277235031-train_loss 0.026103060692548752-valid_loss\n",
            "\n",
            "203-epoch and 0.025774508714675903-train_loss 0.025150274857878685-valid_loss\n",
            "\n",
            "204-epoch and 0.025367826223373413-train_loss 0.026059387251734734-valid_loss\n",
            "\n",
            "205-epoch and 0.025253186002373695-train_loss 0.024982653558254242-valid_loss\n",
            "\n",
            "206-epoch and 0.024705776944756508-train_loss 0.02474767155945301-valid_loss\n",
            "\n",
            "207-epoch and 0.024557016789913177-train_loss 0.024980153888463974-valid_loss\n",
            "\n",
            "208-epoch and 0.024255121126770973-train_loss 0.024733945727348328-valid_loss\n",
            "\n",
            "209-epoch and 0.02405860833823681-train_loss 0.0241326242685318-valid_loss\n",
            "\n",
            "210-epoch and 0.02423497475683689-train_loss 0.02657005749642849-valid_loss\n",
            "\n",
            "211-epoch and 0.025455055758357048-train_loss 0.02630925178527832-valid_loss\n",
            "\n",
            "212-epoch and 0.026489397510886192-train_loss 0.047730207443237305-valid_loss\n",
            "\n",
            "213-epoch and 0.05046960338950157-train_loss 0.13398778438568115-valid_loss\n",
            "\n",
            "214-epoch and 0.14148102700710297-train_loss 0.17492567002773285-valid_loss\n",
            "\n",
            "215-epoch and 0.20538857579231262-train_loss 0.26269030570983887-valid_loss\n",
            "\n",
            "216-epoch and 0.2948460876941681-train_loss 0.22242826223373413-valid_loss\n",
            "\n",
            "217-epoch and 0.25015154480934143-train_loss 0.17381244897842407-valid_loss\n",
            "\n",
            "218-epoch and 0.18996651470661163-train_loss 0.10820204019546509-valid_loss\n",
            "\n",
            "219-epoch and 0.11635284870862961-train_loss 0.06713388860225677-valid_loss\n",
            "\n",
            "220-epoch and 0.07006658613681793-train_loss 0.11811631917953491-valid_loss\n",
            "\n",
            "221-epoch and 0.1281912475824356-train_loss 0.0989561527967453-valid_loss\n",
            "\n",
            "222-epoch and 0.11058975011110306-train_loss 0.06565356999635696-valid_loss\n",
            "\n",
            "223-epoch and 0.07710787653923035-train_loss 0.06393091380596161-valid_loss\n",
            "\n",
            "224-epoch and 0.0762341320514679-train_loss 0.06081226095557213-valid_loss\n",
            "\n",
            "225-epoch and 0.07297816872596741-train_loss 0.054803695529699326-valid_loss\n",
            "\n",
            "226-epoch and 0.06375548243522644-train_loss 0.04583539813756943-valid_loss\n",
            "\n",
            "227-epoch and 0.051086898893117905-train_loss 0.047396790236234665-valid_loss\n",
            "\n",
            "228-epoch and 0.05237220972776413-train_loss 0.04105021804571152-valid_loss\n",
            "\n",
            "229-epoch and 0.0471191443502903-train_loss 0.038348618894815445-valid_loss\n",
            "\n",
            "230-epoch and 0.045323122292757034-train_loss 0.04296598955988884-valid_loss\n",
            "\n",
            "231-epoch and 0.049067676067352295-train_loss 0.034792058169841766-valid_loss\n",
            "\n",
            "232-epoch and 0.040315788239240646-train_loss 0.037419699132442474-valid_loss\n",
            "\n",
            "233-epoch and 0.04462745413184166-train_loss 0.03141201287508011-valid_loss\n",
            "\n",
            "234-epoch and 0.03808360546827316-train_loss 0.03140803426504135-valid_loss\n",
            "\n",
            "235-epoch and 0.037899959832429886-train_loss 0.0326189324259758-valid_loss\n",
            "\n",
            "236-epoch and 0.038129258900880814-train_loss 0.031268663704395294-valid_loss\n",
            "\n",
            "237-epoch and 0.035904500633478165-train_loss 0.030920611694455147-valid_loss\n",
            "\n",
            "238-epoch and 0.03591913357377052-train_loss 0.027394762262701988-valid_loss\n",
            "\n",
            "239-epoch and 0.03303081914782524-train_loss 0.027423622086644173-valid_loss\n",
            "\n",
            "240-epoch and 0.0328730084002018-train_loss 0.026880506426095963-valid_loss\n",
            "\n",
            "241-epoch and 0.03129792958498001-train_loss 0.02685212902724743-valid_loss\n",
            "\n",
            "242-epoch and 0.030810821801424026-train_loss 0.025764992460608482-valid_loss\n",
            "\n",
            "243-epoch and 0.030389724299311638-train_loss 0.023753702640533447-valid_loss\n",
            "\n",
            "244-epoch and 0.02874920517206192-train_loss 0.025335241109132767-valid_loss\n",
            "\n",
            "245-epoch and 0.03053373098373413-train_loss 0.02396717295050621-valid_loss\n",
            "\n",
            "246-epoch and 0.02832942269742489-train_loss 0.02413829043507576-valid_loss\n",
            "\n",
            "247-epoch and 0.027832550927996635-train_loss 0.02460169792175293-valid_loss\n",
            "\n",
            "248-epoch and 0.028170576319098473-train_loss 0.023317093029618263-valid_loss\n",
            "\n",
            "249-epoch and 0.026693711057305336-train_loss 0.02376488409936428-valid_loss\n",
            "\n",
            "250-epoch and 0.02746414579451084-train_loss 0.023044416680932045-valid_loss\n",
            "\n",
            "251-epoch and 0.02745135687291622-train_loss 0.024643367156386375-valid_loss\n",
            "\n",
            "252-epoch and 0.02832946553826332-train_loss 0.022419022396206856-valid_loss\n",
            "\n",
            "253-epoch and 0.02607681229710579-train_loss 0.023158229887485504-valid_loss\n",
            "\n",
            "254-epoch and 0.02693770080804825-train_loss 0.022425901144742966-valid_loss\n",
            "\n",
            "255-epoch and 0.025682931765913963-train_loss 0.022715220227837563-valid_loss\n",
            "\n",
            "256-epoch and 0.02565440535545349-train_loss 0.022111346945166588-valid_loss\n",
            "\n",
            "257-epoch and 0.02539299987256527-train_loss 0.021411433815956116-valid_loss\n",
            "\n",
            "258-epoch and 0.02506423182785511-train_loss 0.02191101759672165-valid_loss\n",
            "\n",
            "259-epoch and 0.02532169036567211-train_loss 0.02207173965871334-valid_loss\n",
            "\n",
            "260-epoch and 0.02506216987967491-train_loss 0.021970603615045547-valid_loss\n",
            "\n",
            "261-epoch and 0.025074297562241554-train_loss 0.021541502326726913-valid_loss\n",
            "\n",
            "262-epoch and 0.02482088841497898-train_loss 0.021688897162675858-valid_loss\n",
            "\n",
            "263-epoch and 0.02477581612765789-train_loss 0.021580249071121216-valid_loss\n",
            "\n",
            "264-epoch and 0.02458679862320423-train_loss 0.02157703787088394-valid_loss\n",
            "\n",
            "265-epoch and 0.024690518155694008-train_loss 0.02149117924273014-valid_loss\n",
            "\n",
            "266-epoch and 0.024403270334005356-train_loss 0.021579481661319733-valid_loss\n",
            "\n",
            "267-epoch and 0.024407587945461273-train_loss 0.021316030994057655-valid_loss\n",
            "\n",
            "268-epoch and 0.024232152849435806-train_loss 0.02183663472533226-valid_loss\n",
            "\n",
            "269-epoch and 0.024217315018177032-train_loss 0.02106192521750927-valid_loss\n",
            "\n",
            "270-epoch and 0.02441079169511795-train_loss 0.021605800837278366-valid_loss\n",
            "\n",
            "271-epoch and 0.02400071918964386-train_loss 0.0214509479701519-valid_loss\n",
            "\n",
            "272-epoch and 0.023812273517251015-train_loss 0.021157873794436455-valid_loss\n",
            "\n",
            "273-epoch and 0.024075517430901527-train_loss 0.024006448686122894-valid_loss\n",
            "\n",
            "274-epoch and 0.025356076657772064-train_loss 0.023424416780471802-valid_loss\n",
            "\n",
            "275-epoch and 0.0282831359654665-train_loss 0.021435029804706573-valid_loss\n",
            "\n",
            "276-epoch and 0.025266768410801888-train_loss 0.023611191660165787-valid_loss\n",
            "\n",
            "277-epoch and 0.026840656995773315-train_loss 0.021769080311059952-valid_loss\n",
            "\n",
            "278-epoch and 0.02524458058178425-train_loss 0.021744752302765846-valid_loss\n",
            "\n",
            "279-epoch and 0.025734668597579002-train_loss 0.021968601271510124-valid_loss\n",
            "\n",
            "280-epoch and 0.025385389104485512-train_loss 0.02249179594218731-valid_loss\n",
            "\n",
            "281-epoch and 0.02523690275847912-train_loss 0.02201518975198269-valid_loss\n",
            "\n",
            "282-epoch and 0.024715539067983627-train_loss 0.02197233773767948-valid_loss\n",
            "\n",
            "283-epoch and 0.024875931441783905-train_loss 0.021773144602775574-valid_loss\n",
            "\n",
            "284-epoch and 0.024767188355326653-train_loss 0.02161259576678276-valid_loss\n",
            "\n",
            "285-epoch and 0.024436140432953835-train_loss 0.02179681323468685-valid_loss\n",
            "\n",
            "286-epoch and 0.02466144785284996-train_loss 0.021263182163238525-valid_loss\n",
            "\n",
            "287-epoch and 0.02434254065155983-train_loss 0.021152213215827942-valid_loss\n",
            "\n",
            "288-epoch and 0.024416916072368622-train_loss 0.02117982693016529-valid_loss\n",
            "\n",
            "289-epoch and 0.0243271142244339-train_loss 0.021297387778759003-valid_loss\n",
            "\n",
            "290-epoch and 0.024220028892159462-train_loss 0.021288493648171425-valid_loss\n",
            "\n",
            "291-epoch and 0.02418164350092411-train_loss 0.021238092333078384-valid_loss\n",
            "\n",
            "292-epoch and 0.02415086142718792-train_loss 0.021196112036705017-valid_loss\n",
            "\n",
            "293-epoch and 0.02406243607401848-train_loss 0.02131393738090992-valid_loss\n",
            "\n",
            "294-epoch and 0.024010058492422104-train_loss 0.021539829671382904-valid_loss\n",
            "\n",
            "295-epoch and 0.02458779700100422-train_loss 0.02134617045521736-valid_loss\n",
            "\n",
            "296-epoch and 0.023921029642224312-train_loss 0.021800367161631584-valid_loss\n",
            "\n",
            "297-epoch and 0.024130571633577347-train_loss 0.021084172651171684-valid_loss\n",
            "\n",
            "298-epoch and 0.02382533624768257-train_loss 0.021367065608501434-valid_loss\n",
            "\n",
            "299-epoch and 0.023903125897049904-train_loss 0.021143687888979912-valid_loss\n",
            "\n",
            "300-epoch and 0.023807771503925323-train_loss 0.021257726475596428-valid_loss\n",
            "\n",
            "301-epoch and 0.02366008795797825-train_loss 0.021956557407975197-valid_loss\n",
            "\n",
            "302-epoch and 0.023977546021342278-train_loss 0.020935073494911194-valid_loss\n",
            "\n",
            "303-epoch and 0.024975193664431572-train_loss 0.021679386496543884-valid_loss\n",
            "\n",
            "304-epoch and 0.024192433804273605-train_loss 0.022029977291822433-valid_loss\n",
            "\n",
            "305-epoch and 0.02435602992773056-train_loss 0.020827552303671837-valid_loss\n",
            "\n",
            "306-epoch and 0.024509167298674583-train_loss 0.020946452394127846-valid_loss\n",
            "\n",
            "307-epoch and 0.02366543933749199-train_loss 0.021907104179263115-valid_loss\n",
            "\n",
            "308-epoch and 0.023970836773514748-train_loss 0.020766334608197212-valid_loss\n",
            "\n",
            "309-epoch and 0.02353162132203579-train_loss 0.020652204751968384-valid_loss\n",
            "\n",
            "310-epoch and 0.023663995787501335-train_loss 0.021482616662979126-valid_loss\n",
            "\n",
            "311-epoch and 0.02362491562962532-train_loss 0.021113697439432144-valid_loss\n",
            "\n",
            "312-epoch and 0.0233524888753891-train_loss 0.02075527422130108-valid_loss\n",
            "\n",
            "313-epoch and 0.023575203493237495-train_loss 0.02096959576010704-valid_loss\n",
            "\n",
            "314-epoch and 0.023241346701979637-train_loss 0.021253805607557297-valid_loss\n",
            "\n",
            "315-epoch and 0.02333415299654007-train_loss 0.020440341904759407-valid_loss\n",
            "\n",
            "316-epoch and 0.02320808731019497-train_loss 0.02043813094496727-valid_loss\n",
            "\n",
            "317-epoch and 0.023154234513640404-train_loss 0.020908763632178307-valid_loss\n",
            "\n",
            "318-epoch and 0.023143667727708817-train_loss 0.020501235499978065-valid_loss\n",
            "\n",
            "319-epoch and 0.022987576201558113-train_loss 0.02039288356900215-valid_loss\n",
            "\n",
            "320-epoch and 0.023019131273031235-train_loss 0.020879799500107765-valid_loss\n",
            "\n",
            "321-epoch and 0.022921094670891762-train_loss 0.020691094920039177-valid_loss\n",
            "\n",
            "322-epoch and 0.0228053480386734-train_loss 0.02029796876013279-valid_loss\n",
            "\n",
            "323-epoch and 0.022812264040112495-train_loss 0.020549355074763298-valid_loss\n",
            "\n",
            "324-epoch and 0.02266683429479599-train_loss 0.020531143993139267-valid_loss\n",
            "\n",
            "325-epoch and 0.022571949288249016-train_loss 0.020132755860686302-valid_loss\n",
            "\n",
            "326-epoch and 0.022533074021339417-train_loss 0.020294571295380592-valid_loss\n",
            "\n",
            "327-epoch and 0.02236003428697586-train_loss 0.02036241628229618-valid_loss\n",
            "\n",
            "328-epoch and 0.022252067923545837-train_loss 0.019956424832344055-valid_loss\n",
            "\n",
            "329-epoch and 0.022189531475305557-train_loss 0.02071508951485157-valid_loss\n",
            "\n",
            "330-epoch and 0.02206863835453987-train_loss 0.01983596384525299-valid_loss\n",
            "\n",
            "331-epoch and 0.02214631251990795-train_loss 0.020533300936222076-valid_loss\n",
            "\n",
            "332-epoch and 0.02179732546210289-train_loss 0.02075817435979843-valid_loss\n",
            "\n",
            "333-epoch and 0.02171662077307701-train_loss 0.01995646394789219-valid_loss\n",
            "\n",
            "334-epoch and 0.02203192375600338-train_loss 0.020986439660191536-valid_loss\n",
            "\n",
            "335-epoch and 0.02161831595003605-train_loss 0.01971030980348587-valid_loss\n",
            "\n",
            "336-epoch and 0.021814873442053795-train_loss 0.020590325817465782-valid_loss\n",
            "\n",
            "337-epoch and 0.021378500387072563-train_loss 0.020087996497750282-valid_loss\n",
            "\n",
            "338-epoch and 0.021173693239688873-train_loss 0.020703181624412537-valid_loss\n",
            "\n",
            "339-epoch and 0.021589970216155052-train_loss 0.020303785800933838-valid_loss\n",
            "\n",
            "340-epoch and 0.023735523223876953-train_loss 0.021008532494306564-valid_loss\n",
            "\n",
            "341-epoch and 0.022081177681684494-train_loss 0.02343658357858658-valid_loss\n",
            "\n",
            "342-epoch and 0.024087974801659584-train_loss 0.020825516432523727-valid_loss\n",
            "\n",
            "343-epoch and 0.024897849187254906-train_loss 0.024173077195882797-valid_loss\n",
            "\n",
            "344-epoch and 0.026543637737631798-train_loss 0.02089596539735794-valid_loss\n",
            "\n",
            "345-epoch and 0.02354789339005947-train_loss 0.020359091460704803-valid_loss\n",
            "\n",
            "346-epoch and 0.023783639073371887-train_loss 0.021080797538161278-valid_loss\n",
            "\n",
            "347-epoch and 0.02323918789625168-train_loss 0.021325454115867615-valid_loss\n",
            "\n",
            "348-epoch and 0.022933218628168106-train_loss 0.020350288599729538-valid_loss\n",
            "\n",
            "349-epoch and 0.022864434868097305-train_loss 0.019883304834365845-valid_loss\n",
            "\n",
            "350-epoch and 0.02197405882179737-train_loss 0.02089243195950985-valid_loss\n",
            "\n",
            "351-epoch and 0.02232387289404869-train_loss 0.020165925845503807-valid_loss\n",
            "\n",
            "352-epoch and 0.02182813547551632-train_loss 0.0200943686068058-valid_loss\n",
            "\n",
            "353-epoch and 0.021822016686201096-train_loss 0.020265908911824226-valid_loss\n",
            "\n",
            "354-epoch and 0.021588128060102463-train_loss 0.020043756812810898-valid_loss\n",
            "\n",
            "355-epoch and 0.021598847582936287-train_loss 0.019181914627552032-valid_loss\n",
            "\n",
            "356-epoch and 0.021081730723381042-train_loss 0.019597962498664856-valid_loss\n",
            "\n",
            "357-epoch and 0.02124018408358097-train_loss 0.019685233011841774-valid_loss\n",
            "\n",
            "358-epoch and 0.021024039015173912-train_loss 0.019094469025731087-valid_loss\n",
            "\n",
            "359-epoch and 0.02086026221513748-train_loss 0.018932020291686058-valid_loss\n",
            "\n",
            "360-epoch and 0.02091154456138611-train_loss 0.019419504329562187-valid_loss\n",
            "\n",
            "361-epoch and 0.020834868773818016-train_loss 0.019391389563679695-valid_loss\n",
            "\n",
            "362-epoch and 0.020588181912899017-train_loss 0.018960094079375267-valid_loss\n",
            "\n",
            "363-epoch and 0.02048051729798317-train_loss 0.019108468666672707-valid_loss\n",
            "\n",
            "364-epoch and 0.020785821601748466-train_loss 0.01923537440598011-valid_loss\n",
            "\n",
            "365-epoch and 0.020438598468899727-train_loss 0.019003264605998993-valid_loss\n",
            "\n",
            "366-epoch and 0.020291345193982124-train_loss 0.018580066040158272-valid_loss\n",
            "\n",
            "367-epoch and 0.020105579867959023-train_loss 0.01883791945874691-valid_loss\n",
            "\n",
            "368-epoch and 0.020264605060219765-train_loss 0.018845614045858383-valid_loss\n",
            "\n",
            "369-epoch and 0.02019796334207058-train_loss 0.019059346988797188-valid_loss\n",
            "\n",
            "370-epoch and 0.020163504406809807-train_loss 0.01852104812860489-valid_loss\n",
            "\n",
            "371-epoch and 0.02018694393336773-train_loss 0.018629837781190872-valid_loss\n",
            "\n",
            "372-epoch and 0.019875433295965195-train_loss 0.01849750615656376-valid_loss\n",
            "\n",
            "373-epoch and 0.01954566314816475-train_loss 0.018303269520401955-valid_loss\n",
            "\n",
            "374-epoch and 0.01939539797604084-train_loss 0.018802406266331673-valid_loss\n",
            "\n",
            "375-epoch and 0.01959086023271084-train_loss 0.018251627683639526-valid_loss\n",
            "\n",
            "376-epoch and 0.019324028864502907-train_loss 0.018262995406985283-valid_loss\n",
            "\n",
            "377-epoch and 0.019169405102729797-train_loss 0.01842433586716652-valid_loss\n",
            "\n",
            "378-epoch and 0.019629336893558502-train_loss 0.01932355761528015-valid_loss\n",
            "\n",
            "379-epoch and 0.020356427878141403-train_loss 0.019028205424547195-valid_loss\n",
            "\n",
            "380-epoch and 0.019517742097377777-train_loss 0.017983878031373024-valid_loss\n",
            "\n",
            "381-epoch and 0.01896006241440773-train_loss 0.019191516563296318-valid_loss\n",
            "\n",
            "382-epoch and 0.019902905449271202-train_loss 0.019095316529273987-valid_loss\n",
            "\n",
            "383-epoch and 0.02082192711532116-train_loss 0.01951151341199875-valid_loss\n",
            "\n",
            "384-epoch and 0.02100960910320282-train_loss 0.01828216202557087-valid_loss\n",
            "\n",
            "385-epoch and 0.019393952563405037-train_loss 0.019541848450899124-valid_loss\n",
            "\n",
            "386-epoch and 0.020778220146894455-train_loss 0.01979985646903515-valid_loss\n",
            "\n",
            "387-epoch and 0.021498389542102814-train_loss 0.018316660076379776-valid_loss\n",
            "\n",
            "388-epoch and 0.019105300307273865-train_loss 0.021464694291353226-valid_loss\n",
            "\n",
            "389-epoch and 0.022273052483797073-train_loss 0.021842174232006073-valid_loss\n",
            "\n",
            "390-epoch and 0.024262407794594765-train_loss 0.020062969997525215-valid_loss\n",
            "\n",
            "391-epoch and 0.021656785160303116-train_loss 0.0232533011585474-valid_loss\n",
            "\n",
            "392-epoch and 0.02390020526945591-train_loss 0.018695369362831116-valid_loss\n",
            "\n",
            "393-epoch and 0.020543085411190987-train_loss 0.019909216091036797-valid_loss\n",
            "\n",
            "394-epoch and 0.021958127617836-train_loss 0.020580347627401352-valid_loss\n",
            "\n",
            "395-epoch and 0.02101060375571251-train_loss 0.019873615354299545-valid_loss\n",
            "\n",
            "396-epoch and 0.020530439913272858-train_loss 0.01858692616224289-valid_loss\n",
            "\n",
            "397-epoch and 0.020748212933540344-train_loss 0.017945369705557823-valid_loss\n",
            "\n",
            "398-epoch and 0.01979779079556465-train_loss 0.018897265195846558-valid_loss\n",
            "\n",
            "399-epoch and 0.020427992567420006-train_loss 0.017909862101078033-valid_loss\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  out = model(data.x_dict, data.edge_index_dict)\n",
        "  pred = out['movie']\n",
        "  print(pred.shape)\n",
        "  #print(pred)\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "  #s = nn.Sigmoid()\n",
        "  #pred = s(pred)\n",
        "  pred = pred > 0.5\n",
        "  pred = pred.float()\n",
        "\n",
        "  count = 0\n",
        "  for i, p in enumerate(pred):\n",
        "    \n",
        "    if p.eq(data['movie'].y[i]).sum().item() == 22 and data['movie'].test_mask[i]: \n",
        "      count+=1\n",
        "      print(i, p,data['movie'].y[i])\n",
        "\n",
        "\n",
        "  #print((pred))\n",
        "  #correct = float (pred[data['movie'].test_mask].eq(data['movie'].y[data['movie'].test_mask]).sum().item())\n",
        "  #acc = correct / data['movie'].test_mask.sum().item()\n",
        "  acc = count / data['movie'].test_mask.sum().item()\n",
        "  print('Accuracy: {:.4f}'.format(acc))\n",
        "\n",
        "  #correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "  #acc = correct / data.test_mask.sum().item()\n",
        "  #print('Accuracy: {:.4f}'.format(acc))"
      ],
      "metadata": {
        "id": "uiB7EbRo6830",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed4ad2d5-059a-41ec-d643-6c4193bf65dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2000, 22])\n",
            "8 tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "9 tensor([0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "11 tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "15 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "25 tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "30 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "37 tensor([0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "52 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "78 tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "114 tensor([1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "117 tensor([1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "123 tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "125 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "136 tensor([0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "137 tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "161 tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "172 tensor([0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "181 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "189 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "199 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "201 tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "214 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "232 tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "240 tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "253 tensor([0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "266 tensor([1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "328 tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "342 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "351 tensor([1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "355 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "357 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "380 tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "388 tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "397 tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "419 tensor([1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "429 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "432 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "434 tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "454 tensor([1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "456 tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "490 tensor([0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "494 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "496 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "504 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "508 tensor([0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "537 tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "547 tensor([0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "550 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "555 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "570 tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "581 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "583 tensor([1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "603 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "613 tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "628 tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "633 tensor([0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "644 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "661 tensor([0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "672 tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "673 tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "681 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "683 tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "687 tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "690 tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "712 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "721 tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "736 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "749 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "751 tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "767 tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "769 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "779 tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "792 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "801 tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "803 tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "805 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "815 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "822 tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "835 tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "847 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "861 tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "885 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "887 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "901 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "922 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "935 tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "944 tensor([1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "947 tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "950 tensor([1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "968 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "975 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "976 tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "978 tensor([0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "984 tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "998 tensor([0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1005 tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1006 tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1025 tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1033 tensor([0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1043 tensor([1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1067 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1072 tensor([0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1103 tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1107 tensor([1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1151 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1165 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1191 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1194 tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1199 tensor([0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1205 tensor([0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1213 tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1224 tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1251 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1263 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1264 tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1273 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1307 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1332 tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1347 tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1368 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1396 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1410 tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1411 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1450 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1480 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1484 tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1487 tensor([0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1489 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1490 tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1493 tensor([1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1555 tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1600 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1605 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1614 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1647 tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1652 tensor([0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1655 tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1662 tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1663 tensor([1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1674 tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1703 tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1704 tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1733 tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1736 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1737 tensor([0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1740 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1741 tensor([1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1744 tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1755 tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1775 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1781 tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1791 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1800 tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1811 tensor([1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1817 tensor([1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1823 tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1830 tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1833 tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1834 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1840 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1845 tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1855 tensor([0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1867 tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1869 tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1896 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1898 tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1900 tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1904 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1925 tensor([1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1930 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1933 tensor([1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1935 tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1936 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1958 tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1966 tensor([1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1967 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1968 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1969 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1974 tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "1983 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.], device='cuda:0') tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "Accuracy: 0.9000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot([i for i in range(len(losses))], losses)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('BCEWithLogitsLoss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "cEVv4MvmmwTb",
        "outputId": "4d5e1ec4-544f-40e4-a6d4-5f846196717a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.plot([i for i in range(len(losses2))], losses2)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('BCEWithLogitsLoss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "atmjP1e1bQav",
        "outputId": "7da9645d-b905-4613-9c59-6f22add1ce68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  out = model(data.x_dict, data.edge_index_dict)\n",
        "  pred = out['movie'].cpu().numpy()\n",
        "  print(pred.shape)\n",
        "  np.save('./embed_movie_learned', pred)"
      ],
      "metadata": {
        "id": "MDd6vq5OeRBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qdRiSLP-ofm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_load = np.load('./embed_movie_learned.npy')\n",
        "result = torch.from_numpy(np_load)\n",
        "print(result.shape)\n",
        "print(result[0])"
      ],
      "metadata": {
        "id": "wGEMHTCfeqjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.utils.convert import to_networkx\n",
        "\n",
        "dataset1 = Planetoid(root = '/content/cora',name='Cora')\n",
        "\n",
        "cora = dataset1 [0]\n",
        "\n",
        "coragraph = to_networkx(cora)\n",
        "\n",
        "node_labels = cora.y[list(coragraph.nodes)].numpy()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(1,figsize=(14,12)) \n",
        "nx.draw(coragraph, cmap=plt.get_cmap('Set1'),node_color = node_labels,node_size=75,linewidths=6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ic5tla50oVFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((dataset1[0]))"
      ],
      "metadata": {
        "id": "Wcod40GgESbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# class target 정보 제외\n",
        "train_df = data[\"movie\"].x\n",
        "\n",
        "# 2차원 t-SNE 임베딩\n",
        "tsne_np = TSNE(n_components = 2).fit_transform(train_df)\n",
        "\n",
        "# numpy array -> DataFrame 변환\n",
        "tsne_df = pd.DataFrame(tsne_np, columns = ['component 0', 'component 1'])\n",
        "\n"
      ],
      "metadata": {
        "id": "VlJmDZSEFVCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.cm as cm\n",
        "\n",
        "color_labels = pd.read_csv(path+\"/label_genre.txt\", sep=\"\\t\")\n",
        "colors = cm.rainbow(np.linspace(0, 1, 357))\n",
        "\n",
        "for i, c in enumerate(color_labels[\"label\"]):\n",
        "  plt.scatter(tsne_df['component 0'][i], tsne_df['component 1'][i], color = colors[c], label = str(c))\n",
        "  #if i == 50: break\n",
        "\n",
        "plt.figure(1,figsize=(14,12)) \n",
        "plt.xlabel('component 0')\n",
        "plt.ylabel('component 1')\n",
        "#plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "80APi7cU204f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.cm as cm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "path = \"/content/drive/MyDrive/MovieRecommendation/GAT/data\"\n",
        "nontrain = torch.from_numpy(np.load('/content/drive/MyDrive/MovieRecommendation/GAT/data/embed_movie_learned.npy'))\n",
        "# class target 정보 제외\n",
        "train_df = nontrain\n",
        "# 2차원 t-SNE 임베딩\n",
        "tsne_np = TSNE(n_components = 2).fit_transform(train_df)\n",
        "\n",
        "# numpy array -> DataFrame 변환\n",
        "tsne_df = pd.DataFrame(tsne_np, columns = ['component 0', 'component 1'])\n",
        "\n",
        "\n",
        "\n",
        "color_labels = pd.read_csv(path+\"/label_genre.txt\", sep=\"\\t\")\n",
        "colors = cm.rainbow(np.linspace(0, 1, 357))\n",
        "\n",
        "for i, c in enumerate(color_labels[\"label\"]):\n",
        "  plt.scatter(tsne_df['component 0'][i], tsne_df['component 1'][i], color = colors[c], label = str(c))\n",
        "  #if i == 50: break\n",
        "\n",
        "plt.figure(1,figsize=(14,12)) \n",
        "plt.xlabel('component 0')\n",
        "plt.ylabel('component 1')\n",
        "#plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EZcKggLx9K8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gat 예제\n"
      ],
      "metadata": {
        "id": "zB2fzzBFMFXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "dataset = Planetoid(root='/tmp/Cora', name='Cora')"
      ],
      "metadata": {
        "id": "Syrl3CGqIeIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "befor = dataset[0]['edge_index'].detach().clone()\n",
        "print(dataset[0])"
      ],
      "metadata": {
        "id": "2BYR6Bo_MkRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
        "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "GtfAIitbMEkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "data = dataset[0].to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print('loss: {:.4f}'.format(loss))"
      ],
      "metadata": {
        "id": "vUNTyRNVMNu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "_, pred = model(data).max(dim=1)\n",
        "correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "acc = correct / data.test_mask.sum().item()\n",
        "print('Accuracy: {:.4f}'.format(acc))"
      ],
      "metadata": {
        "id": "i24x_YahMRzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.equal(dataset[0]['edge_index'], befor))"
      ],
      "metadata": {
        "id": "E9NENMz3MyMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vRQ-YS27SXXM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}